# -*- coding: utf-8 -*-
"""text_classification(Naive).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1E6vM2CJkxfCxlZz_zBP8vlIDbilIrQbI

#import library
"""

import nltk
import re

# download dataset
nltk.download('twitter_samples')
from nltk.corpus import twitter_samples


#text normalization
import nltk
nltk.download('stopwords')
import nltk
nltk.download('punkt')

#Lemmatizer
nltk.download('wordnet')
from nltk.stem import WordNetLemmatizer

"""# Read Data"""

positive_tweets=twitter_samples.strings('positive_tweets.json')
negative_tweets=twitter_samples.strings('negative_tweets.json')

#print(len(positive_tweets))
#print(len(negative_tweets))

"""# Text Preprocessing"""

def clean_text(tweet):
    #clean text
    tweet=re.sub('(#|@)\w*',"",tweet)# \w [a-z|A_Z|0-9|_] #remoce hashtage ,username
    tweet=re.sub("https?:\/\/\S+","",tweet) #remove hyperlink
    tweet=re.sub("(\?|!)+"," ",tweet) #remve (?!)
    tweet=re.sub("\s\d+\s","",tweet) # 33
    tweet=re.sub("(\.|\,)+","",tweet) #remove . ,
    tweet=re.sub("^\s+","",tweet) #remove space ^ >> start of string
    tweet=re.sub("\s+$","",tweet)#remove space  $ >> at the end of the string
    tweet=re.sub(":","",tweet)
    tweet=re.sub("[_:()\\\]","",tweet)

    return tweet

from nltk.corpus import stopwords
stop_words=stopwords.words('english')

def process_sentence(tweets):
    clean_tweets=[]
    for tweet in tweets:
        tweet=clean_text(tweet)
        tweet=nltk.word_tokenize(tweet) #sequncing
        c_tweet=[word.lower() for word in tweet if word.lower() not in stop_words] #remove stop wods & convert to lower case

        #lemmatizer
        lemmatizer = WordNetLemmatizer()
        clean_tweet=[lemmatizer.lemmatize(word) for word in c_tweet]#convert word to  base

        clean_tweets.append(clean_tweet)

    return clean_tweets

positive_tweets=process_sentence(positive_tweets)
negative_tweets=process_sentence(negative_tweets)

#for i in range(10):
#    print(positive_tweets[i])

positive_labels=[1]*len(positive_tweets)
negative_labels=[0]*len(negative_tweets)
positive_labels.extend(negative_labels)
positive_tweets.extend(negative_tweets)

labels=positive_labels
tweets=positive_tweets

#print("len tweets",len(tweets))
#print("len labels",len(labels))

#shuffle two lists
import random

zip_list=list(zip(tweets,labels))
random.shuffle(zip_list)
tweets,labels=zip(*zip_list)

#tweets[1]

#labels[1]

"""## Build freq table"""

def build_freq(tweets,labels):
    freq={}

    #iterat tweets
    for i in range(len(tweets)):
        #iterate on each word in each tweets
        for word in tweets[i]:
            key=word
            if key not in freq.keys():### frist time
                if labels[i]==1:
                    freq[key]=[1,0]#positive class
                else:
                    freq[key]=[0,1]#negative class
            else:
                if labels[i]==1:### exist before
                    freq[key][0]+=1
                else:
                    freq[key][1]+=1

    return freq

freq_table=build_freq(tweets,labels)

#freq_table

sum_pos_freq=0
sum_neg_freq=0
for key in freq_table.keys():
    sum_pos_freq+=freq_table[key][0]
    sum_neg_freq+=freq_table[key][1]

#print(sum_pos_freq,sum_neg_freq)

V=len(freq_table)
#print("number of unique words in vocabulary (V) : ",V)

def build_propability(freq_table,sum_pos_freq,sum_neg_freq,V):
    prop_dict={}

    #key :word ,value:[p(w/pos),p(w/neg)]
    for key in freq_table.keys():
        prop_dict[key]=[((freq_table[key][0]+1)/(sum_pos_freq+V)),((freq_table[key][1]+1)/(sum_neg_freq+V))]

    return prop_dict

prop_dict=build_propability(freq_table,sum_pos_freq,sum_neg_freq,V)

#prop_dict

import numpy as np
def Naive_Bayes_inference(tweets,prop_dict):
    results=[]

    for tweet in tweets:

        result=0
        for word in tweet:
            try:
                result+=np.log(prop_dict[word][0]/prop_dict[word][1])# not multiply of prior ration because the tweets are balanced

            except:
                result+=0 # if the word is not defined, zero not make a probelem because it is sum not multiply
        results.append(result)
    return results

"""## evaluate Model"""

y_pred=Naive_Bayes_inference(tweets,prop_dict)

y_p=[1 if y>=0 else 0 for y in y_pred]

from sklearn.metrics import accuracy_score
print("Accuracy Score : ",accuracy_score(y_p,labels))

"""## Make Prediction"""

tweets[1],labels[1]

Naive_Bayes_inference([tweets[1]],prop_dict)

#negative probability mean that negative tweets.

"""#Deployment

"""

test_tweets=["@metalgear_jp @Kojima_Hideo I want you're T-shirts ! They are so cool ! :D","Stats for the day have arrived. 2 new followers and NO unfollowers :) via http://t.co/xxlXs6xYwe.",
             "Dang that is some rad @AbzuGame #fanart! :D https://t.co/bI8k8tb9ht","Can u feel it? :((:( #exo http://t.co/ghsa262ORm","@seanactual You mean you're not offering? :("]

clean_tweet=process_sentence(test_tweets)
#clean_tweet

pred=Naive_Bayes_inference(clean_tweet,prop_dict)
#pred

#frist three tweets is positive and last two are negative.